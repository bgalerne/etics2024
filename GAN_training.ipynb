{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Ml5gAj8EaJr8"
      ],
      "authorship_tag": "ABX9TyNohv78ododdhTVggb97x88",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bgalerne/etics2024/blob/main/GAN_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV1iu3g19SgB"
      },
      "source": [
        "# Generative Adversarial Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that gpu is available:"
      ],
      "metadata": {
        "id": "vSCcqbJRwpUJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFR78hGKPlx9"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3-nug6USbT5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr8c4V7LU2uN"
      },
      "source": [
        "#GAN MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a69JR0dKRqJM"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Load MNIST and define some function to view images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2IkxQGJU8JM"
      },
      "source": [
        "transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "datatrain = datasets.MNIST('.', train=True, download=True, transform=transform)\n",
        "batch_size = 100\n",
        "trainloader = torch.utils.data.DataLoader(datatrain,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True)\n",
        "\n",
        "def imshow(img):\n",
        "    img = img*0.5 + 0.5     # unnormalize\n",
        "    pil_img = torchvision.transforms.functional.to_pil_image(img)\n",
        "    display(pil_img)\n",
        "    print(\"Image size (w x h): \",  pil_img.height, \"x\", pil_img.width)\n",
        "    return(pil_img)\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "print('Images:')\n",
        "imshow(torchvision.utils.make_grid(images, nrow=10))\n",
        "# print labels\n",
        "print('Labels:')\n",
        "for i in range(10):\n",
        "  print(' '.join('%5s' % str(labels[10*i+j].numpy()) for j in range(10)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNytRCdMYhQX"
      },
      "source": [
        "## Generative and discriminative models:\n",
        "\n",
        "**Exercise**  \n",
        "Define the Generator and discriminative models following the above instructions:\n",
        "\n",
        "*Generator*  \n",
        "Define a class `G_net` with the following architecture:\n",
        "\n",
        "Generator (input : a random vector in the latent space of dimension $k$)\n",
        "$k$ is a parameter of the constructor see below.  \n",
        "The network applies the following list operations:\n",
        "1. Fully connected layer, output size 256\n",
        "2. Leaky ReLU ($\\alpha = 0.2$) activation\n",
        "3. Fully connected layer, output size 512\n",
        "4. Leaky ReLU ($\\alpha = 0.2$) activation\n",
        "5. Fully connected layer, output size 784\n",
        "6. Tanh activation\n",
        "7. Reshape (1x28Ã—28) (use `torch.reshape`)\n",
        "\n",
        "*Discriminator*  \n",
        "Define a class `D_net` with the following architecture:\n",
        "1. Flatten images to 1D tensors of size 784 = 28*28.\n",
        "2. Fully connected layer, output size 512\n",
        "3. Leaky ReLU ($\\alpha = 0.2$) activation\n",
        "4. Fully connected layer, output size 256\n",
        "5. Leaky ReLU ($\\alpha = 0.2$) activation\n",
        "6. Fully connected layer, output size 1\n",
        "\n",
        "Define instances of the networks using $k=32$.\n",
        "\n",
        "**Remark:** One should add a sigmoid layer to the discriminator to output a probability but this is implicitly done in the criterion (see next exercise).\n",
        "\n",
        "See https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#define-a-convolutional-neural-network for an example of neural network definition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZDqEuBSY1Of"
      },
      "source": [
        "# Generator network:\n",
        "class G_Net(nn.Module):\n",
        "  def __init__(self, k):\n",
        "    super(G_Net, self).__init__()\n",
        "  #TODO\n",
        "\n",
        "  def forward(self,x):\n",
        "  #TODO\n",
        "\n",
        "\n",
        "# Discriminator network:\n",
        "class D_Net(nn.Module):\n",
        "  #TODO\n",
        "\n",
        "k = 32\n",
        "G_net = G_Net(k).to(device)\n",
        "D_net = D_Net().to(device)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGSzc-Jgc5CO"
      },
      "source": [
        "## View some generated images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8bGzguyc9h9"
      },
      "source": [
        "def show_G_net(z=None):\n",
        "  # provide random latent code as option to see evolution\n",
        "  with torch.no_grad():\n",
        "    if z==None:\n",
        "      z = torch.randn(100,k).to(device)\n",
        "    genimages = G_net(z)\n",
        "    pil_img = imshow(torchvision.utils.make_grid(genimages.to('cpu'),nrow=10))\n",
        "    return(pil_img)\n",
        "\n",
        "show_G_net()\n",
        "zshow = torch.randn(100,k).to(device)\n",
        "\n",
        "# reproducible:\n",
        "show_G_net(zshow)\n",
        "\n",
        "show_G_net(zshow)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPoe6I3-eHai"
      },
      "source": [
        "## GAN training code\n",
        "\n",
        "In PyTorch,\n",
        "the function `nn.BCEWithLogitsLoss` combines a `Sigmoid` layer and the `BCELoss`, that is,  for $(x,y)\\in\\mathbb{R}\\times \\{0,1\\}$,\n",
        "$$\n",
        "\\ell(x,y) =  -y \\cdot \\log \\sigma(x)\n",
        "        - (1 - y) \\cdot \\log (1 - \\sigma(x))\n",
        "$$\n",
        "where $\\sigma: \\mathbb{R}\\to (0,1)$ is the sigmoid function defined by\n",
        "$$\n",
        "\\sigma(x) = \\frac{e^x}{1+e^{x}} = \\frac{1}{1+e^{-x}}.\n",
        "$$\n",
        "The sigmoid function plays the role of the `softmax` function for binary classification since it maps $\\mathbb{R}\\to (0,1)$ to produce the probability of being in the class $y=1$ (and then $1 - \\sigma(x)$ is the probability of being in the class $y=0$).\n",
        "\n",
        "In the course formula of the discriminator loss,\n",
        "$$\n",
        "      \\max_{\\theta_d}\n",
        "      \\underbrace{\n",
        "        \\sum_{x_{\\text{real}} \\in \\mathcal{T}_{\\text{real}}} \\log D_{\\theta_d}(x_{\\text{real}})}_{\n",
        "        \\substack{\n",
        "          \\text{force predicted labels to be 1}\\\\\n",
        "          \\text{for real images}\n",
        "        }\n",
        "      }\n",
        "      +\n",
        "      \\underbrace{\n",
        "        \\sum_{x_{\\text{fake}} \\in \\mathcal{T}_{\\text{fake}}} \\log (1 - D_{\\theta_d}(x_{\\text{fake}}))}_{\n",
        "        \\substack{\n",
        "          \\text{force predicted labels to be 0}\\\\\n",
        "          \\text{for fake images}\n",
        "        }}\n",
        "$$\n",
        "the sigmoid layer is implicitly included in $D_{\\theta_d}$, but this will not be the case in the PyTorch implementation.\n",
        "In short,\n",
        "$$\n",
        "D_{\\theta_d}(x) = \\sigma(\\mathtt{D\\_{net}}(x)).\n",
        "$$\n",
        "\n",
        "**Exercise**\n",
        "\n",
        "Implement the following training algorithm, where $b$ is the batch size (and we take $m=n=b$ in the loss formula):\n",
        "\n",
        "\n",
        "> For each batch of images $x_{\\text{real}}$:\n",
        "> > **1) Train discriminator:**\n",
        "> > > Generate $z$ a tensor of size $b\\times k$ of idd Gaussian variables  \n",
        "> > > Generate  $x_{\\text{fake}} = \\mathtt{G\\_{net}}(z)$ a set of $b$ fake images  \n",
        "> > > Compute the (opposite of the) loss to minimize for the discriminator using `nn.BCEWithLogitsLoss`\n",
        "> > >  \n",
        "> > > Compute the gradient and do an optimizer step for the disciminator parameters  \n",
        "\n",
        "> > **2) Train the generator:**\n",
        "> > > Generate $z$ a new tensor of size $b\\times k$ of idd Gaussian variables  \n",
        "> > > Compute the loss to minimize\n",
        "$$\n",
        "      \\underbrace{\n",
        "        -\n",
        "        \\sum_{z \\in \\mathcal T_{\\text{rand}}} \\log D_{\\theta_d}(G_{\\theta_g}(z)))\n",
        "      }_{\n",
        "        \\substack{\n",
        "          \\text{force the discriminator to think that}\\\\\n",
        "          \\text{our generated fake images are real (close to 1)}\n",
        "        }\n",
        "      }\n",
        "$$\n",
        "using `nn.BCEWithLogitsLoss`  \n",
        "Compute the gradient and do an optimizer step for the generator parameters\n",
        "\n",
        "Train the networks for 20 epochs using batch size $b=100$.\n",
        "Display the current losses and show generated images at the end of each epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J-h9XnZeGrz"
      },
      "source": [
        "\n",
        "D_lr = 0.0002\n",
        "G_lr = 0.0002\n",
        "G_optimizer = optim.Adam(G_net.parameters(), lr=D_lr,betas=(0.5, 0.999))\n",
        "D_optimizer = optim.Adam(D_net.parameters(), lr=G_lr,betas=(0.5, 0.999))\n",
        "\n",
        "# mode collapse with SGD:\n",
        "#G_optimizer = optim.SGD(G_net.parameters(), lr=D_lr, momentum=0.9)\n",
        "#D_optimizer = optim.SGD(D_net.parameters(), lr=G_lr, momentum=0.9)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6CIY-YHfMq5"
      },
      "source": [
        "num_epochs = 20\n",
        "\n",
        "# binary class arrays\n",
        "bsones = torch.ones((batch_size,1), dtype=torch.float).to(device)\n",
        "bszeros = torch.zeros((batch_size,1), dtype=torch.float).to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for batch_idx, data in enumerate(trainloader,0):\n",
        "    # train discriminator:\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    # D_loss = ???\n",
        "\n",
        "    # train generator:\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    # G_loss = ???\n",
        "\n",
        "  # end of epoch:\n",
        "  print('Train Epoch: {} [{}/{} ({:.0f}%)]  DisLoss: {:.6e} GenLoss: {:.6e}'.format(\n",
        "      epoch, (batch_idx+1) * batch_size, len(trainloader.dataset),\n",
        "      100. * (batch_idx+1) / len(trainloader), D_loss.item(), G_loss.item() ))\n",
        "  # show 100 generated images with same latent z for each epoch:\n",
        "  show_G_net(zshow)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXbp5DXM__0K"
      },
      "source": [
        "## Interpolation in latent space:\n",
        "\n",
        "**Exercise:**\n",
        "\n",
        "Generate 2 sets of 10 latent variable $z_0$ and $z_1$ and display the generated images by the latent variables:\n",
        "$$\n",
        "z_\\theta = (1-\\theta) z_0 + \\theta z_1\n",
        "$$\n",
        "for $\\theta$ varying between $0$ and $1$ (using the `torch.linspace` function with 20 intermediate values).\n",
        "Display all the images in a grid of height 10 and width 20 images.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lATPnHSTAFIP"
      },
      "source": [
        "with torch.no_grad():\n",
        "  nlatent = 10\n",
        "  ninterp = 20\n",
        "  z0 = torch.randn(nlatent,k).to(device)\n",
        "  z1 = torch.randn(nlatent,k).to(device)\n",
        "\n",
        "  #TODO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mode collapse:\n",
        "\n",
        "**Exercise:**\n",
        "Observe than when switching to SGD instead of Adam mode collapse, the generator suffers of mode collapse."
      ],
      "metadata": {
        "id": "bLvHp7xsv79W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOzoqzzE5oku"
      },
      "source": [
        "#Sources:\n",
        "Load MNIST: https://github.com/pytorch/examples/blob/master/mnist/main.py  \n",
        "GAN architecture: TP GAN by Alasdair Newson: https://sites.google.com/site/alasdairnewson/teaching.  \n",
        "Another more complex model: https://github.com/RAKIYOU/Training-GAN-on-MNIST"
      ]
    }
  ]
}